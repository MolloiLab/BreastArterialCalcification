{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This julia notebook shows how to load the model and apply it to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/Project BAC/BAC project/libs`\n",
      "┌ Warning: MPI Implementation is not CUDA Aware.\n",
      "└ @ FluxMPI /home/molloi-lab/.julia/packages/FluxMPI/BwbGS/src/FluxMPI.jl:28\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"/home/molloi-lab/Desktop/Project BAC/BAC project/libs/\")\n",
    "using Lux, Random, NNlib, Zygote, LuxCUDA, CUDA, FluxMPI, JLD2, DICOM\n",
    "using Images\n",
    "using MLUtils\n",
    "using MPI\n",
    "using Optimisers\n",
    "using ImageMorphology, ChainRulesCore, Statistics, CSV, DataFrames, Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.allowscalar(false)\n",
    "\n",
    "FluxMPI.Init(;gpu_devices = [0,1,2,3])\n",
    "\n",
    "# Rank(similar to threadID) of the current process.\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = MPI.Comm_rank(comm)\n",
    "dev = gpu_device()\n",
    "dev_cpu = cpu_device()\n",
    "\n",
    "_conv = (in, out) -> Conv((3, 3), in=>out, pad=1)\n",
    "conv1 = (in, out) -> Chain(_conv(in, out), BatchNorm(out, leakyrelu))\n",
    "\n",
    "_tran = (in, out) -> ConvTranspose((2, 2), in => out, stride = 2)\n",
    "tran = (in, out) -> Chain(_tran(in, out), BatchNorm(out, leakyrelu))\n",
    "\n",
    "struct UNet{\n",
    "    CH1, CH2, CH3, CH4, CH5, CH6, CH7\n",
    "} <: Lux.AbstractExplicitContainerLayer{\n",
    "    (:l1, :l2, :l3, :l4, :l5, :l6, :l7)\n",
    "}\n",
    "    l1::CH1\n",
    "    l2::CH2\n",
    "    l3::CH3\n",
    "    l4::CH4\n",
    "    l5::CH5\n",
    "    l6::CH6\n",
    "    l7::CH7\n",
    "end\n",
    "\n",
    "function UNet(in_chs, lbl_chs, size)\n",
    "    l1 = Chain(conv1(in_chs, size), conv1(size, size))\n",
    "    l2 = Chain(MaxPool((2,2), stride=2), conv1(size, size*2), conv1(size*2, size*2))\n",
    "    l3 = Chain(MaxPool((2,2), stride=2), conv1(size*2, size*4), conv1(size*4, size*4))\n",
    "    l4 = Chain(MaxPool((2,2), stride=2), conv1(size*4, size*8), conv1(size*8, size*8), tran(size*8, size*4))\n",
    "\n",
    "    # Expanding layers\n",
    "    l5 = Chain(conv1(size*8, size*4), conv1(size*4, size*4), tran(size*4, size*2))\n",
    "    l6 = Chain(conv1(size*4, size*2), conv1(size*2, size*2), tran(size*2, size))\n",
    "    l7 = Chain(conv1(size*2, size), conv1(size, size), Conv((1, 1), size=>lbl_chs), sigmoid)\n",
    "\n",
    "    UNet(l1, l2, l3, l4, l5, l6, l7)\n",
    "end\n",
    "\n",
    "function (m::UNet)(x, ps, st::NamedTuple)\n",
    "    # Convolutional layers\n",
    "    x1, st_l1 = m.l1(x, ps.l1, st.l1)\n",
    "\n",
    "    x2, st_l2 = m.l2(x1, ps.l2, st.l2)\n",
    "\n",
    "    # Downscaling Blocks\n",
    "    x3, st_l3 = m.l3(x2, ps.l3, st.l3)\n",
    "    x4, st_l4 = m.l4(x3, ps.l4, st.l4)\n",
    "\n",
    "    # Upscaling Blocks\n",
    "    x5, st_l5 = m.l5(cat(x4, x3; dims=3), ps.l5, st.l5)\n",
    "    x6, st_l6 = m.l6(cat(x5, x2; dims=3), ps.l6, st.l6)\n",
    "    x7, st_l7 = m.l7(cat(x6, x1; dims=3), ps.l7, st.l7)\n",
    "\n",
    "\n",
    "    # Merge states\n",
    "    st = (\n",
    "    l1=st_l1, l2=st_l2, l3=st_l3, l4=st_l4, l5=st_l5, l6=st_l6, l7=st_l7\n",
    "    )\n",
    "\n",
    "    return x7, st\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/media/molloi-lab/2TB1/Clean_Dataset_full/SID-100510/L_CC.3328_2560.dcm\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input data\n",
    "input_img = \"/media/molloi-lab/2TB1/Clean_Dataset_full/SID-100510/L_CC.3328_2560.dcm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the image\n",
    "# ground_truth_mask = Float32.(Images.load(ground_truth_mask_path))\n",
    "# breast_mask = Float32.(Images.load(breast_mask_path))\n",
    "# dcm_data = dcm_parse(dcm_path)\n",
    "# is_reversed = uppercase(dcm_data[(0x2050, 0x0020)]) == \"INVERSE\"\n",
    "# pixel_size = dcm_data[(0x0018, 0x1164)]\n",
    "# img = Float32.(dcm_data[(0x7fe0, 0x0010)])\n",
    "# original_size = size(img)\n",
    "# # resize image based on pixel length\n",
    "# img, breast_mask, ground_truth_mask, new_size = resize_dicom_image(img, breast_mask, ground_truth_mask, pixel_size)\n",
    "# # normalize image and correct color\n",
    "# img = normalize_img(img; mask = breast_mask, invert = is_reversed)\n",
    "# # crop to breast only\n",
    "# img_cropped, ground_truth_mask_cropped, coords = crop_to_bounding_box(breast_mask, img, ground_truth_mask)\n",
    "# # save resize info to local\n",
    "# @save joinpath(curr_dir, f_name*\"_resize_info.jld2\") original_size new_size coords\n",
    "# # check size\n",
    "# x, y = size(img_cropped)\n",
    "# # if y % 32 != 0\n",
    "# #     x_org, y_org = size(img)\n",
    "# #     println(i, \"\\t\", ct+1)\n",
    "# #     println(\"($x_org, $y_org)\")\n",
    "# #     println(\"($x, $y)\\n\")\n",
    "# # end\n",
    "# @assert x % 32 == 0\n",
    "# @assert y % 32 == 0\n",
    "\n",
    "# #save\n",
    "# @save joinpath(out_dir, f_name*\".jld2\") img_cropped\n",
    "# Images.save(joinpath(out_dir, f_name*\".png\"), Gray.(round.(ground_truth_mask_cropped)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(l1 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-1.5754029, 2.2304332, -1.6438724, 2.1291182, 1.5642703, -4.459931, -1.4128605, 2.5394652, 0.08220151, 1.3944055, -2.2787423, 2.0576603, -1.6820405, 1.7552352, 2.6440036, -2.1005633], running_var = Float32[2.0897603, 2.1679688, 0.8702852, 0.7614294, 0.4340817, 9.882541, 1.1594844, 3.4043326, 0.6023198, 1.5117741, 2.6206417, 2.2926908, 1.1610765, 1.3491561, 1.8846543, 1.7653261], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-0.26003373, 0.75799036, 1.0921711, -2.363193, -2.6438394, 0.91217846, -11.351362, 1.798894, -0.120637566, -1.7964456, -2.17908, 0.97645134, -0.6907514, -0.715033, 0.84746337, 1.8764355], running_var = Float32[0.86277986, 2.34481, 0.5159975, 3.5334196, 2.3837838, 4.866938, 86.0603, 4.1092596, 0.39124367, 4.560316, 4.7320285, 2.6757293, 1.6857156, 1.596671, 2.1655583, 3.9297612], training = Val{true}())), l2 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[1.5154504, -1.9864492, -0.05900887, -1.8279338, -0.45725176, -1.7132331, -1.5774999, -1.6919795, -1.8124511, -1.2015193  …  -1.8943713, 0.8022662, -0.28530863, -5.601649, 0.1532071, 0.05182159, -0.6386317, -2.187148, -0.25315025, -3.1455092], running_var = Float32[9.6278515, 3.6874192, 1.7821552, 7.6003714, 7.908434, 4.0281296, 2.8905642, 5.1883016, 3.7529757, 4.8467627  …  6.0086713, 2.4655778, 2.5662675, 23.759802, 5.0576425, 5.8578525, 3.6530638, 11.6891575, 2.382889, 6.4362507], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-0.38670686, -3.4821405, 0.8948534, -7.470035, -0.3325103, -0.80308604, -0.8082723, -1.1250818, -4.366359, 0.020487003  …  -2.1897147, -4.647575, -0.8263539, -1.5992923, 5.158841, 0.285445, -5.0593944, 0.25321457, -1.6346327, -0.30238426], running_var = Float32[13.25464, 13.936933, 10.781684, 31.032028, 5.566, 7.282704, 17.183945, 20.748896, 15.620119, 5.12627  …  8.335145, 17.327988, 8.0336, 6.9813347, 6.6565514, 3.8190966, 32.76575, 12.917389, 11.410164, 7.671706], training = Val{true}())), l3 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[-4.5070324, -3.6086025, -3.3104753, -1.2870357, -3.6962695, 2.478482, -3.0897262, -2.072947, -0.92573875, -3.5626233  …  -0.8161328, -2.3371005, -3.2450821, -0.86261946, -0.29698133, -1.2981201, -9.703284, -1.4329699, -1.0966451, 3.011059], running_var = Float32[15.987832, 10.599855, 12.930322, 35.928932, 11.445602, 10.5548725, 28.689053, 15.338634, 6.3653245, 8.0531845  …  8.627871, 9.290679, 22.849585, 7.559071, 6.0918703, 6.8685684, 54.55773, 8.032581, 11.159955, 12.744537], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-3.1847692, -4.411163, 0.13701688, -2.1381001, 4.199629, -4.199776, -3.7285619, 3.5002022, -5.1926847, 3.2604406  …  -2.4859455, -3.3416178, -2.9819596, 0.2907448, -1.8212411, -3.7625725, -1.4776698, -2.7399633, -0.7259112, -4.45209], running_var = Float32[27.021809, 29.903276, 18.154123, 16.25652, 39.594326, 23.51737, 24.635033, 29.815945, 15.189225, 18.707788  …  15.680192, 23.456427, 19.503086, 26.018862, 16.511292, 18.987051, 19.733242, 17.137589, 11.813095, 20.573347], training = Val{true}())), l4 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[-4.6018972, -6.4119287, -0.6601016, -3.972513, 0.73298705, 0.32276353, -2.5916796, -0.9283724, 8.84205, -3.5314968  …  -2.7598455, -4.5599008, -10.949865, -3.4567327, 0.4007349, -2.0541868, -6.2357907, -6.5835023, -4.220362, -1.7432785], running_var = Float32[12.170371, 16.407602, 11.219261, 10.239388, 9.127764, 7.601739, 11.995673, 9.809753, 30.651272, 10.435767  …  10.799029, 9.061483, 30.719759, 13.132886, 8.461739, 9.636921, 19.636986, 14.243659, 16.243866, 13.581226], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-1.2959343, -1.2909658, 0.16663474, -4.2599816, -2.3000445, 3.570889, -2.4836326, -2.0251374, -1.7749065, -3.108264  …  -0.39329255, -2.1871207, -2.068814, -3.9509804, -2.0536783, -0.43730664, -3.0139768, -0.100325495, -3.9822783, -4.7487917], running_var = Float32[12.871855, 10.011332, 11.883709, 27.74107, 9.609573, 11.742779, 9.470422, 9.121495, 9.6405325, 21.981926  …  10.593099, 9.594905, 17.374401, 15.45239, 15.388648, 18.774017, 13.540326, 10.264322, 9.410594, 41.94588], training = Val{true}()), layer_6 = NamedTuple(), layer_7 = (running_mean = Float32[-0.8075587, -0.35784167, -0.3256879, 0.8113212, -1.0499439, -0.4906547, -1.0537039, -0.9429849, -1.6440116, -0.8446082  …  -0.98818624, -1.2170913, -1.0054722, -1.1656944, -0.8214088, -0.8918874, -0.46898264, -1.0942973, -1.3845578, -1.2448634], running_var = Float32[1.7556516, 2.4620602, 1.573752, 1.8793017, 1.8735964, 1.3580947, 1.2465026, 1.463065, 2.7665656, 1.7498118  …  1.5953432, 1.7570844, 1.9325224, 1.6127695, 1.5213866, 2.4579513, 1.8512408, 1.520702, 1.6887246, 2.6519449], training = Val{true}())), l5 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-1.7049261, -12.799857, -10.7579775, -2.3393955, -5.3553457, -11.133736, -11.088706, -0.24853665, 2.0720465, -6.1234565  …  -3.1254816, -24.402235, -8.899295, -11.349597, -12.9706545, -11.474875, -8.14318, -14.117596, -3.7789216, -7.6217227], running_var = Float32[31.714796, 45.48835, 17.298887, 14.214306, 15.716167, 45.40012, 48.47154, 46.425404, 19.051628, 13.661215  …  36.58498, 134.77783, 24.090263, 84.66989, 24.387972, 29.898108, 19.604326, 96.977005, 21.688875, 18.152905], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-5.969445, 11.112707, -2.297911, 0.2532852, -6.8239374, -3.639218, -1.8662655, -1.6908175, -2.38462, -2.9967182  …  5.9437532, -10.00198, 3.062419, -6.4528995, -3.8245046, -2.4728782, -8.332784, -2.7674334, -3.5066917, -9.648895], running_var = Float32[19.060806, 15.844644, 16.522848, 10.065914, 25.591347, 6.1295753, 6.6551595, 16.034666, 5.864401, 16.201351  …  27.601461, 34.51728, 9.647924, 14.70416, 35.08426, 7.278095, 35.32991, 7.394416, 13.488609, 20.409946], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = (running_mean = Float32[-1.5234184, -1.2186186, -0.02490592, -0.78027123, -0.34944618, -0.7902519, -0.95135164, -0.6897209, 0.2480168, -1.0789561  …  -0.91608727, -0.14998516, -0.2579602, -0.36708957, 0.11641896, -0.22830302, -0.27364036, -1.0914941, -0.3476661, -0.90150803], running_var = Float32[1.728971, 1.2626207, 2.2535167, 1.2932254, 0.9994264, 0.52282625, 1.4398298, 0.77620417, 0.9257943, 1.4324213  …  1.0207062, 0.73044306, 1.0497355, 2.0980537, 0.8629615, 0.64879704, 0.9555804, 1.6620896, 1.3908074, 1.1800174], training = Val{true}())), l6 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.45398077, 3.1040728, -4.1734366, -9.457641, -4.385779, 0.29065302, 11.094552, -11.427861, -11.377348, -1.7000929  …  -3.0779853, -10.214769, -12.478619, -4.480768, -7.092387, -8.477194, -0.53878605, -3.0072377, -10.478661, -13.429768], running_var = Float32[24.1616, 37.0673, 21.684593, 24.465296, 11.908242, 33.754585, 27.94479, 67.092, 30.679657, 14.620921  …  15.834817, 76.85379, 97.55973, 31.168755, 19.895134, 19.94756, 18.443316, 29.511322, 71.37834, 79.24699], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-7.350101, -1.3057345, 0.09133152, 0.69866395, 0.20496458, 2.4354463, -3.1037986, -1.6975197, 2.4328735, 5.2691216  …  -4.399921, -6.570023, 0.09587215, -0.15653865, -0.42224, -0.32776442, -2.9438632, 0.8235666, 1.9659805, 3.46391], running_var = Float32[49.17146, 9.221922, 9.571911, 4.2467732, 9.096215, 11.094487, 19.29322, 19.021053, 8.27593, 4.2129154  …  17.527779, 73.72455, 11.014356, 9.312718, 16.919437, 14.305601, 21.291338, 8.370148, 5.4509015, 10.0234995], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = (running_mean = Float32[-0.88177675, 0.74388593, 0.79187936, -0.5618399, 0.22302414, -0.041371685, 0.6301633, 0.17459732, -0.5959811, -0.91441065, 0.48147509, -0.11453826, 0.79867584, -0.9572144, 0.6485394, 0.83113277], running_var = Float32[0.83086854, 0.5760305, 0.5823655, 1.2953343, 0.46762547, 0.75991756, 0.5340705, 0.71516967, 0.575326, 0.8368994, 0.73958755, 0.5081083, 0.38439253, 0.8508277, 0.64039004, 0.6822942], training = Val{true}())), l7 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.9361402, -1.8351213, -0.6547662, -6.233516, 3.905557, 3.791931, 2.694274, 3.806114, 0.86151266, 4.210085, 4.489805, -6.0706887, 2.493828, -2.8929956, 0.9997623, 0.21269886], running_var = Float32[57.14985, 42.12395, 13.565391, 37.170547, 14.667648, 17.328228, 22.091482, 55.127613, 42.91621, 20.877857, 40.242283, 33.235554, 32.552547, 98.26823, 9.651412, 48.53098], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-2.4264116, -1.5976502, -0.7153492, -0.7808589, -0.7240819, -3.690629, -0.6527047, -0.41899863, 0.115406215, 3.0912266, -0.22297582, -3.053255, -2.1707454, -2.9569552, -0.06372799, -3.8138742], running_var = Float32[5.8451667, 3.2155938, 15.752311, 63.16103, 76.92424, 6.4162664, 51.739735, 62.66319, 43.013035, 38.877594, 14.477288, 11.7569685, 5.209212, 4.4188404, 89.966095, 5.273845], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = NamedTuple()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the trained model\n",
    "model_path = \"/home/molloi-lab/Desktop/wenbo2_flashdrive_backup/saved_train_info_334.jld2\"\n",
    "@load model_path ps_save st_save\n",
    "ps_save = ps_save |> dev\n",
    "st_save = st_save |> dev\n",
    "\n",
    "# ps = FluxMPI.synchronize!(ps; root_rank = 0)\n",
    "# st = FluxMPI.synchronize!(st; root_rank = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "    l1 = Chain(\n",
       "        layer_1 = Conv((3, 3), 1 => 16, pad=1),  \u001b[90m# 160 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 16 => 16, pad=1),  \u001b[90m# 2_320 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "    ),\n",
       "    l2 = Chain(\n",
       "        layer_1 = MaxPool((2, 2)),\n",
       "        layer_2 = Conv((3, 3), 16 => 32, pad=1),  \u001b[90m# 4_640 parameters\u001b[39m\n",
       "        layer_3 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "        layer_4 = Conv((3, 3), 32 => 32, pad=1),  \u001b[90m# 9_248 parameters\u001b[39m\n",
       "        layer_5 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "    ),\n",
       "    l3 = Chain(\n",
       "        layer_1 = MaxPool((2, 2)),\n",
       "        layer_2 = Conv((3, 3), 32 => 64, pad=1),  \u001b[90m# 18_496 parameters\u001b[39m\n",
       "        layer_3 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "        layer_4 = Conv((3, 3), 64 => 64, pad=1),  \u001b[90m# 36_928 parameters\u001b[39m\n",
       "        layer_5 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "    ),\n",
       "    l4 = Chain(\n",
       "        layer_1 = MaxPool((2, 2)),\n",
       "        layer_2 = Conv((3, 3), 64 => 128, pad=1),  \u001b[90m# 73_856 parameters\u001b[39m\n",
       "        layer_3 = BatchNorm(128, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 257\u001b[39m\n",
       "        layer_4 = Conv((3, 3), 128 => 128, pad=1),  \u001b[90m# 147_584 parameters\u001b[39m\n",
       "        layer_5 = BatchNorm(128, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 257\u001b[39m\n",
       "        layer_6 = ConvTranspose((2, 2), 128 => 64, stride=2),  \u001b[90m# 32_832 parameters\u001b[39m\n",
       "        layer_7 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "    ),\n",
       "    l5 = Chain(\n",
       "        layer_1 = Conv((3, 3), 128 => 64, pad=1),  \u001b[90m# 73_792 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 64 => 64, pad=1),  \u001b[90m# 36_928 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "        layer_5 = ConvTranspose((2, 2), 64 => 32, stride=2),  \u001b[90m# 8_224 parameters\u001b[39m\n",
       "        layer_6 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "    ),\n",
       "    l6 = Chain(\n",
       "        layer_1 = Conv((3, 3), 64 => 32, pad=1),  \u001b[90m# 18_464 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 32 => 32, pad=1),  \u001b[90m# 9_248 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "        layer_5 = ConvTranspose((2, 2), 32 => 16, stride=2),  \u001b[90m# 2_064 parameters\u001b[39m\n",
       "        layer_6 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "    ),\n",
       "    l7 = Chain(\n",
       "        layer_1 = Conv((3, 3), 32 => 16, pad=1),  \u001b[90m# 4_624 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 16 => 16, pad=1),  \u001b[90m# 2_320 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "        layer_5 = Conv((1, 1), 16 => 1),  \u001b[90m# 17 parameters\u001b[39m\n",
       "        layer_6 = WrappedFunction(σ),\n",
       "    ),\n",
       ") \u001b[90m        # Total: \u001b[39m483_377 parameters,\n",
       "\u001b[90m          #        plus \u001b[39m1_649 states."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = UNet(1, 1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ, st = Lux.apply(model, x, ps, st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
