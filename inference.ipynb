{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This julia notebook shows how to load the model and apply it to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/Project BAC/BAC project/libs`\n",
      "┌ Warning: MPI Implementation is not CUDA Aware.\n",
      "└ @ FluxMPI /home/molloi-lab/.julia/packages/FluxMPI/BwbGS/src/FluxMPI.jl:28\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"/home/molloi-lab/Desktop/Project BAC/BAC project/libs/\")\n",
    "using Lux, Random, NNlib, Zygote, LuxCUDA, CUDA, FluxMPI, JLD2, DICOM\n",
    "using Images\n",
    "using MLUtils\n",
    "using MPI\n",
    "using Optimisers\n",
    "using ImageMorphology, ChainRulesCore, Statistics, CSV, DataFrames, Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.allowscalar(false)\n",
    "\n",
    "FluxMPI.Init(;gpu_devices = [0,1,2,3])\n",
    "\n",
    "# Rank(similar to threadID) of the current process.\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = MPI.Comm_rank(comm)\n",
    "dev = gpu_device()\n",
    "dev_cpu = cpu_device()\n",
    "\n",
    "_conv = (in, out) -> Conv((3, 3), in=>out, pad=1)\n",
    "conv1 = (in, out) -> Chain(_conv(in, out), BatchNorm(out, leakyrelu))\n",
    "\n",
    "_tran = (in, out) -> ConvTranspose((2, 2), in => out, stride = 2)\n",
    "tran = (in, out) -> Chain(_tran(in, out), BatchNorm(out, leakyrelu))\n",
    "\n",
    "struct UNet{\n",
    "    CH1, CH2, CH3, CH4, CH5, CH6, CH7\n",
    "} <: Lux.AbstractExplicitContainerLayer{\n",
    "    (:l1, :l2, :l3, :l4, :l5, :l6, :l7)\n",
    "}\n",
    "    l1::CH1\n",
    "    l2::CH2\n",
    "    l3::CH3\n",
    "    l4::CH4\n",
    "    l5::CH5\n",
    "    l6::CH6\n",
    "    l7::CH7\n",
    "end\n",
    "\n",
    "function UNet(in_chs, lbl_chs, size)\n",
    "    l1 = Chain(conv1(in_chs, size), conv1(size, size))\n",
    "    l2 = Chain(MaxPool((2,2), stride=2), conv1(size, size*2), conv1(size*2, size*2))\n",
    "    l3 = Chain(MaxPool((2,2), stride=2), conv1(size*2, size*4), conv1(size*4, size*4))\n",
    "    l4 = Chain(MaxPool((2,2), stride=2), conv1(size*4, size*8), conv1(size*8, size*8), tran(size*8, size*4))\n",
    "\n",
    "    # Expanding layers\n",
    "    l5 = Chain(conv1(size*8, size*4), conv1(size*4, size*4), tran(size*4, size*2))\n",
    "    l6 = Chain(conv1(size*4, size*2), conv1(size*2, size*2), tran(size*2, size))\n",
    "    l7 = Chain(conv1(size*2, size), conv1(size, size), Conv((1, 1), size=>lbl_chs), sigmoid)\n",
    "\n",
    "    UNet(l1, l2, l3, l4, l5, l6, l7)\n",
    "end\n",
    "\n",
    "function (m::UNet)(x, ps, st::NamedTuple)\n",
    "    # Convolutional layers\n",
    "    x1, st_l1 = m.l1(x, ps.l1, st.l1)\n",
    "\n",
    "    x2, st_l2 = m.l2(x1, ps.l2, st.l2)\n",
    "\n",
    "    # Downscaling Blocks\n",
    "    x3, st_l3 = m.l3(x2, ps.l3, st.l3)\n",
    "    x4, st_l4 = m.l4(x3, ps.l4, st.l4)\n",
    "\n",
    "    # Upscaling Blocks\n",
    "    x5, st_l5 = m.l5(cat(x4, x3; dims=3), ps.l5, st.l5)\n",
    "    x6, st_l6 = m.l6(cat(x5, x2; dims=3), ps.l6, st.l6)\n",
    "    x7, st_l7 = m.l7(cat(x6, x1; dims=3), ps.l7, st.l7)\n",
    "\n",
    "\n",
    "    # Merge states\n",
    "    st = (\n",
    "    l1=st_l1, l2=st_l2, l3=st_l3, l4=st_l4, l5=st_l5, l6=st_l6, l7=st_l7\n",
    "    )\n",
    "\n",
    "    return x7, st\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/media/molloi-lab/2TB1/Clean_Dataset_full/SID-100510/L_CC.3328_2560.dcm\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input data\n",
    "input_img = \"/media/molloi-lab/2TB1/Clean_Dataset_full/SID-100510/L_CC.3328_2560.dcm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the image\n",
    "ground_truth_mask = Float32.(Images.load(ground_truth_mask_path))\n",
    "breast_mask = Float32.(Images.load(breast_mask_path))\n",
    "dcm_data = dcm_parse(dcm_path)\n",
    "is_reversed = uppercase(dcm_data[(0x2050, 0x0020)]) == \"INVERSE\"\n",
    "pixel_size = dcm_data[(0x0018, 0x1164)]\n",
    "img = Float32.(dcm_data[(0x7fe0, 0x0010)])\n",
    "original_size = size(img)\n",
    "# resize image based on pixel length\n",
    "img, breast_mask, ground_truth_mask, new_size = resize_dicom_image(img, breast_mask, ground_truth_mask, pixel_size)\n",
    "# normalize image and correct color\n",
    "img = normalize_img(img; mask = breast_mask, invert = is_reversed)\n",
    "# crop to breast only\n",
    "img_cropped, ground_truth_mask_cropped, coords = crop_to_bounding_box(breast_mask, img, ground_truth_mask)\n",
    "# save resize info to local\n",
    "@save joinpath(curr_dir, f_name*\"_resize_info.jld2\") original_size new_size coords\n",
    "# check size\n",
    "x, y = size(img_cropped)\n",
    "# if y % 32 != 0\n",
    "#     x_org, y_org = size(img)\n",
    "#     println(i, \"\\t\", ct+1)\n",
    "#     println(\"($x_org, $y_org)\")\n",
    "#     println(\"($x, $y)\\n\")\n",
    "# end\n",
    "@assert x % 32 == 0\n",
    "@assert y % 32 == 0\n",
    "\n",
    "#save\n",
    "@save joinpath(out_dir, f_name*\".jld2\") img_cropped\n",
    "Images.save(joinpath(out_dir, f_name*\".png\"), Gray.(round.(ground_truth_mask_cropped)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(l1 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-1.5754029, 2.2304332, -1.6438724, 2.1291182, 1.5642703, -4.459931, -1.4128605, 2.5394652, 0.08220151, 1.3944055, -2.2787423, 2.0576603, -1.6820405, 1.7552352, 2.6440036, -2.1005633], running_var = Float32[2.0897603, 2.1679688, 0.8702852, 0.7614294, 0.4340817, 9.882541, 1.1594844, 3.4043326, 0.6023198, 1.5117741, 2.6206417, 2.2926908, 1.1610765, 1.3491561, 1.8846543, 1.7653261], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-0.26003373, 0.75799036, 1.0921711, -2.363193, -2.6438394, 0.91217846, -11.351362, 1.798894, -0.120637566, -1.7964456, -2.17908, 0.97645134, -0.6907514, -0.715033, 0.84746337, 1.8764355], running_var = Float32[0.86277986, 2.34481, 0.5159975, 3.5334196, 2.3837838, 4.866938, 86.0603, 4.1092596, 0.39124367, 4.560316, 4.7320285, 2.6757293, 1.6857156, 1.596671, 2.1655583, 3.9297612], training = Val{true}())), l2 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[1.5154504, -1.9864492, -0.05900887, -1.8279338, -0.45725176, -1.7132331, -1.5774999, -1.6919795, -1.8124511, -1.2015193  …  -1.8943713, 0.8022662, -0.28530863, -5.601649, 0.1532071, 0.05182159, -0.6386317, -2.187148, -0.25315025, -3.1455092], running_var = Float32[9.6278515, 3.6874192, 1.7821552, 7.6003714, 7.908434, 4.0281296, 2.8905642, 5.1883016, 3.7529757, 4.8467627  …  6.0086713, 2.4655778, 2.5662675, 23.759802, 5.0576425, 5.8578525, 3.6530638, 11.6891575, 2.382889, 6.4362507], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-0.38670686, -3.4821405, 0.8948534, -7.470035, -0.3325103, -0.80308604, -0.8082723, -1.1250818, -4.366359, 0.020487003  …  -2.1897147, -4.647575, -0.8263539, -1.5992923, 5.158841, 0.285445, -5.0593944, 0.25321457, -1.6346327, -0.30238426], running_var = Float32[13.25464, 13.936933, 10.781684, 31.032028, 5.566, 7.282704, 17.183945, 20.748896, 15.620119, 5.12627  …  8.335145, 17.327988, 8.0336, 6.9813347, 6.6565514, 3.8190966, 32.76575, 12.917389, 11.410164, 7.671706], training = Val{true}())), l3 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[-4.5070324, -3.6086025, -3.3104753, -1.2870357, -3.6962695, 2.478482, -3.0897262, -2.072947, -0.92573875, -3.5626233  …  -0.8161328, -2.3371005, -3.2450821, -0.86261946, -0.29698133, -1.2981201, -9.703284, -1.4329699, -1.0966451, 3.011059], running_var = Float32[15.987832, 10.599855, 12.930322, 35.928932, 11.445602, 10.5548725, 28.689053, 15.338634, 6.3653245, 8.0531845  …  8.627871, 9.290679, 22.849585, 7.559071, 6.0918703, 6.8685684, 54.55773, 8.032581, 11.159955, 12.744537], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-3.1847692, -4.411163, 0.13701688, -2.1381001, 4.199629, -4.199776, -3.7285619, 3.5002022, -5.1926847, 3.2604406  …  -2.4859455, -3.3416178, -2.9819596, 0.2907448, -1.8212411, -3.7625725, -1.4776698, -2.7399633, -0.7259112, -4.45209], running_var = Float32[27.021809, 29.903276, 18.154123, 16.25652, 39.594326, 23.51737, 24.635033, 29.815945, 15.189225, 18.707788  …  15.680192, 23.456427, 19.503086, 26.018862, 16.511292, 18.987051, 19.733242, 17.137589, 11.813095, 20.573347], training = Val{true}())), l4 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[-4.6018972, -6.4119287, -0.6601016, -3.972513, 0.73298705, 0.32276353, -2.5916796, -0.9283724, 8.84205, -3.5314968  …  -2.7598455, -4.5599008, -10.949865, -3.4567327, 0.4007349, -2.0541868, -6.2357907, -6.5835023, -4.220362, -1.7432785], running_var = Float32[12.170371, 16.407602, 11.219261, 10.239388, 9.127764, 7.601739, 11.995673, 9.809753, 30.651272, 10.435767  …  10.799029, 9.061483, 30.719759, 13.132886, 8.461739, 9.636921, 19.636986, 14.243659, 16.243866, 13.581226], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-1.2959343, -1.2909658, 0.16663474, -4.2599816, -2.3000445, 3.570889, -2.4836326, -2.0251374, -1.7749065, -3.108264  …  -0.39329255, -2.1871207, -2.068814, -3.9509804, -2.0536783, -0.43730664, -3.0139768, -0.100325495, -3.9822783, -4.7487917], running_var = Float32[12.871855, 10.011332, 11.883709, 27.74107, 9.609573, 11.742779, 9.470422, 9.121495, 9.6405325, 21.981926  …  10.593099, 9.594905, 17.374401, 15.45239, 15.388648, 18.774017, 13.540326, 10.264322, 9.410594, 41.94588], training = Val{true}()), layer_6 = NamedTuple(), layer_7 = (running_mean = Float32[-0.8075587, -0.35784167, -0.3256879, 0.8113212, -1.0499439, -0.4906547, -1.0537039, -0.9429849, -1.6440116, -0.8446082  …  -0.98818624, -1.2170913, -1.0054722, -1.1656944, -0.8214088, -0.8918874, -0.46898264, -1.0942973, -1.3845578, -1.2448634], running_var = Float32[1.7556516, 2.4620602, 1.573752, 1.8793017, 1.8735964, 1.3580947, 1.2465026, 1.463065, 2.7665656, 1.7498118  …  1.5953432, 1.7570844, 1.9325224, 1.6127695, 1.5213866, 2.4579513, 1.8512408, 1.520702, 1.6887246, 2.6519449], training = Val{true}())), l5 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-1.7049261, -12.799857, -10.7579775, -2.3393955, -5.3553457, -11.133736, -11.088706, -0.24853665, 2.0720465, -6.1234565  …  -3.1254816, -24.402235, -8.899295, -11.349597, -12.9706545, -11.474875, -8.14318, -14.117596, -3.7789216, -7.6217227], running_var = Float32[31.714796, 45.48835, 17.298887, 14.214306, 15.716167, 45.40012, 48.47154, 46.425404, 19.051628, 13.661215  …  36.58498, 134.77783, 24.090263, 84.66989, 24.387972, 29.898108, 19.604326, 96.977005, 21.688875, 18.152905], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-5.969445, 11.112707, -2.297911, 0.2532852, -6.8239374, -3.639218, -1.8662655, -1.6908175, -2.38462, -2.9967182  …  5.9437532, -10.00198, 3.062419, -6.4528995, -3.8245046, -2.4728782, -8.332784, -2.7674334, -3.5066917, -9.648895], running_var = Float32[19.060806, 15.844644, 16.522848, 10.065914, 25.591347, 6.1295753, 6.6551595, 16.034666, 5.864401, 16.201351  …  27.601461, 34.51728, 9.647924, 14.70416, 35.08426, 7.278095, 35.32991, 7.394416, 13.488609, 20.409946], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = (running_mean = Float32[-1.5234184, -1.2186186, -0.02490592, -0.78027123, -0.34944618, -0.7902519, -0.95135164, -0.6897209, 0.2480168, -1.0789561  …  -0.91608727, -0.14998516, -0.2579602, -0.36708957, 0.11641896, -0.22830302, -0.27364036, -1.0914941, -0.3476661, -0.90150803], running_var = Float32[1.728971, 1.2626207, 2.2535167, 1.2932254, 0.9994264, 0.52282625, 1.4398298, 0.77620417, 0.9257943, 1.4324213  …  1.0207062, 0.73044306, 1.0497355, 2.0980537, 0.8629615, 0.64879704, 0.9555804, 1.6620896, 1.3908074, 1.1800174], training = Val{true}())), l6 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.45398077, 3.1040728, -4.1734366, -9.457641, -4.385779, 0.29065302, 11.094552, -11.427861, -11.377348, -1.7000929  …  -3.0779853, -10.214769, -12.478619, -4.480768, -7.092387, -8.477194, -0.53878605, -3.0072377, -10.478661, -13.429768], running_var = Float32[24.1616, 37.0673, 21.684593, 24.465296, 11.908242, 33.754585, 27.94479, 67.092, 30.679657, 14.620921  …  15.834817, 76.85379, 97.55973, 31.168755, 19.895134, 19.94756, 18.443316, 29.511322, 71.37834, 79.24699], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-7.350101, -1.3057345, 0.09133152, 0.69866395, 0.20496458, 2.4354463, -3.1037986, -1.6975197, 2.4328735, 5.2691216  …  -4.399921, -6.570023, 0.09587215, -0.15653865, -0.42224, -0.32776442, -2.9438632, 0.8235666, 1.9659805, 3.46391], running_var = Float32[49.17146, 9.221922, 9.571911, 4.2467732, 9.096215, 11.094487, 19.29322, 19.021053, 8.27593, 4.2129154  …  17.527779, 73.72455, 11.014356, 9.312718, 16.919437, 14.305601, 21.291338, 8.370148, 5.4509015, 10.0234995], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = (running_mean = Float32[-0.88177675, 0.74388593, 0.79187936, -0.5618399, 0.22302414, -0.041371685, 0.6301633, 0.17459732, -0.5959811, -0.91441065, 0.48147509, -0.11453826, 0.79867584, -0.9572144, 0.6485394, 0.83113277], running_var = Float32[0.83086854, 0.5760305, 0.5823655, 1.2953343, 0.46762547, 0.75991756, 0.5340705, 0.71516967, 0.575326, 0.8368994, 0.73958755, 0.5081083, 0.38439253, 0.8508277, 0.64039004, 0.6822942], training = Val{true}())), l7 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.9361402, -1.8351213, -0.6547662, -6.233516, 3.905557, 3.791931, 2.694274, 3.806114, 0.86151266, 4.210085, 4.489805, -6.0706887, 2.493828, -2.8929956, 0.9997623, 0.21269886], running_var = Float32[57.14985, 42.12395, 13.565391, 37.170547, 14.667648, 17.328228, 22.091482, 55.127613, 42.91621, 20.877857, 40.242283, 33.235554, 32.552547, 98.26823, 9.651412, 48.53098], training = Val{true}()), layer_3 = NamedTuple(), layer_4 = (running_mean = Float32[-2.4264116, -1.5976502, -0.7153492, -0.7808589, -0.7240819, -3.690629, -0.6527047, -0.41899863, 0.115406215, 3.0912266, -0.22297582, -3.053255, -2.1707454, -2.9569552, -0.06372799, -3.8138742], running_var = Float32[5.8451667, 3.2155938, 15.752311, 63.16103, 76.92424, 6.4162664, 51.739735, 62.66319, 43.013035, 38.877594, 14.477288, 11.7569685, 5.209212, 4.4188404, 89.966095, 5.273845], training = Val{true}()), layer_5 = NamedTuple(), layer_6 = NamedTuple()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the trained model\n",
    "model_path = \"/home/molloi-lab/Desktop/wenbo2_flashdrive_backup/saved_train_info_334.jld2\"\n",
    "@load model_path ps_save st_save\n",
    "ps_save = ps_save |> dev\n",
    "st_save = st_save |> dev\n",
    "\n",
    "# ps = FluxMPI.synchronize!(ps; root_rank = 0)\n",
    "# st = FluxMPI.synchronize!(st; root_rank = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "    l1 = Chain(\n",
       "        layer_1 = Conv((3, 3), 1 => 16, pad=1),  \u001b[90m# 160 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 16 => 16, pad=1),  \u001b[90m# 2_320 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "    ),\n",
       "    l2 = Chain(\n",
       "        layer_1 = MaxPool((2, 2)),\n",
       "        layer_2 = Conv((3, 3), 16 => 32, pad=1),  \u001b[90m# 4_640 parameters\u001b[39m\n",
       "        layer_3 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "        layer_4 = Conv((3, 3), 32 => 32, pad=1),  \u001b[90m# 9_248 parameters\u001b[39m\n",
       "        layer_5 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "    ),\n",
       "    l3 = Chain(\n",
       "        layer_1 = MaxPool((2, 2)),\n",
       "        layer_2 = Conv((3, 3), 32 => 64, pad=1),  \u001b[90m# 18_496 parameters\u001b[39m\n",
       "        layer_3 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "        layer_4 = Conv((3, 3), 64 => 64, pad=1),  \u001b[90m# 36_928 parameters\u001b[39m\n",
       "        layer_5 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "    ),\n",
       "    l4 = Chain(\n",
       "        layer_1 = MaxPool((2, 2)),\n",
       "        layer_2 = Conv((3, 3), 64 => 128, pad=1),  \u001b[90m# 73_856 parameters\u001b[39m\n",
       "        layer_3 = BatchNorm(128, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 257\u001b[39m\n",
       "        layer_4 = Conv((3, 3), 128 => 128, pad=1),  \u001b[90m# 147_584 parameters\u001b[39m\n",
       "        layer_5 = BatchNorm(128, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 257\u001b[39m\n",
       "        layer_6 = ConvTranspose((2, 2), 128 => 64, stride=2),  \u001b[90m# 32_832 parameters\u001b[39m\n",
       "        layer_7 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "    ),\n",
       "    l5 = Chain(\n",
       "        layer_1 = Conv((3, 3), 128 => 64, pad=1),  \u001b[90m# 73_792 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 64 => 64, pad=1),  \u001b[90m# 36_928 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(64, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 129\u001b[39m\n",
       "        layer_5 = ConvTranspose((2, 2), 64 => 32, stride=2),  \u001b[90m# 8_224 parameters\u001b[39m\n",
       "        layer_6 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "    ),\n",
       "    l6 = Chain(\n",
       "        layer_1 = Conv((3, 3), 64 => 32, pad=1),  \u001b[90m# 18_464 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 32 => 32, pad=1),  \u001b[90m# 9_248 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(32, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 65\u001b[39m\n",
       "        layer_5 = ConvTranspose((2, 2), 32 => 16, stride=2),  \u001b[90m# 2_064 parameters\u001b[39m\n",
       "        layer_6 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "    ),\n",
       "    l7 = Chain(\n",
       "        layer_1 = Conv((3, 3), 32 => 16, pad=1),  \u001b[90m# 4_624 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "        layer_3 = Conv((3, 3), 16 => 16, pad=1),  \u001b[90m# 2_320 parameters\u001b[39m\n",
       "        layer_4 = BatchNorm(16, leakyrelu, affine=true, track_stats=true),  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 33\u001b[39m\n",
       "        layer_5 = Conv((1, 1), 16 => 1),  \u001b[90m# 17 parameters\u001b[39m\n",
       "        layer_6 = WrappedFunction(σ),\n",
       "    ),\n",
       ") \u001b[90m        # Total: \u001b[39m483_377 parameters,\n",
       "\u001b[90m          #        plus \u001b[39m1_649 states."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = UNet(1, 1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ, st = Lux.apply(model, x, ps, st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "function zoom_pixel_values(img; mask=nothing)\n",
    "    selection = (mask == nothing) ? nothing : findall(isone, mask)\n",
    "    # Determine if we're working on the whole image or a selection\n",
    "    working_img = (selection !== nothing) ? img[selection] : img\n",
    "    \n",
    "    a, b = minimum(working_img), maximum(working_img)\n",
    "    img_ = (img .- a) ./ (b - a)\n",
    "    \n",
    "    if mask !== nothing\n",
    "        img_ = img_ .* mask\n",
    "    end\n",
    "\n",
    "    return img_\n",
    "end\n",
    "\n",
    "\n",
    "function histogram_equalization(img; mask=nothing)\n",
    "    selection = (mask == nothing) ? nothing : findall(isone, mask)\n",
    "    img = zoom_pixel_values(img; mask = mask)\n",
    "    # Determine if we're working on the whole image or a selection\n",
    "    working_img = (selection !== nothing) ? img[selection] : img\n",
    "    len = length(working_img)\n",
    "    \n",
    "    # Initialize histogram and cumulative histogram\n",
    "    nbins = 256\n",
    "    hist = zeros(Int, nbins)\n",
    "    chist = zeros(Int, nbins)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    for val in working_img\n",
    "        bin = Int(floor(val * (nbins - 1)) + 1)\n",
    "        hist[bin] += 1\n",
    "    end\n",
    "    \n",
    "    # Compute the cumulative histogram\n",
    "    chist[1] = hist[1]\n",
    "    for i in 2:nbins\n",
    "        chist[i] = chist[i - 1] + hist[i]\n",
    "    end\n",
    "    \n",
    "    # Perform histogram equalization\n",
    "    min_chist = minimum(filter(x -> x > 0, chist))\n",
    "    total_pixels = len\n",
    "    new_img = copy(img)\n",
    "    \n",
    "    indices = (selection !== nothing) ? selection : 1:length(img)\n",
    "    for i in indices\n",
    "        bin = Int(floor(img[i] * (nbins - 1)) + 1)\n",
    "        new_intensity = (chist[bin] - min_chist) / (total_pixels - min_chist)\n",
    "        new_img[i] = Float32(new_intensity)\n",
    "    end\n",
    "    \n",
    "    if mask !== nothing\n",
    "        new_img = new_img .* mask\n",
    "    end\n",
    "\n",
    "    return new_img\n",
    "end\n",
    "\n",
    "\n",
    "# function xlogy(x, y)\n",
    "#     result = x * log(y)\n",
    "#     return ifelse(iszero(x), zero(result), result)\n",
    "# end\n",
    "\n",
    "function weighted_xlogy(x, y, weight)\n",
    "    result = weight * x * log(y)\n",
    "    return ifelse(iszero(x), zero(result), result)\n",
    "end\n",
    "\n",
    "function weighted_bce_loss(y, ŷ; w_pos = 25 , w_neg = 1, ϵ=1f-5)\n",
    "    return mean(@. -(weighted_xlogy(y, ŷ .+ ϵ, w_pos) + weighted_xlogy(1f0 .- y, 1f0 .- ŷ .+ ϵ, w_neg)))\n",
    "end\n",
    "\n",
    "\n",
    "function bce_dice_and_hd_loss(ŷ,  y, epoch, step, save_output; HD_kick_in = 501, ϵ=1f-5)\n",
    "    # x_size, y_size, _, num_batches = size(ŷ)\n",
    "    # hd_weight = min(75f-2, max(0, epoch-HD_kick_in)*5f-3) # starts at epoch#51, max at epoch#200\n",
    "    # hd_factor = 1.3332f-3 * hd_weight + 1.0f-7\n",
    "    # dice\n",
    "    loss_dice = 1f0 - (muladd(2f0, sum(ŷ .* y), ϵ) / (sum(ŷ .^ 2) + sum(y .^ 2) + ϵ))\n",
    "\n",
    "    # binarycrossentropy\n",
    "    # bce_loss = mean(@. -xlogy(y, ŷ .+ ϵ) - xlogy(1f0 .- y, 1f0 .- ŷ .+ ϵ))\n",
    "    # bce_loss = mean(@. -xlogy(y, ŷ + ϵ) - xlogy(1f0 - y, 1f0 - ŷ + ϵ))\n",
    "    bce_loss = weighted_bce_loss(y, ŷ)\n",
    "\n",
    "    # # HD\n",
    "    # ŷ_dtm = fill(1f3, (x_size, y_size, 1, num_batches))\n",
    "    # y_dtm = fill(1f3, (x_size, y_size, 1, num_batches))\n",
    "    # ŷ_cpu = ŷ |> dev_cpu\n",
    "    # y_cpu = y |> dev_cpu\n",
    "    # # FluxMPI.fluxmpi_println(extrema(ŷ_cpu))\n",
    "    # ignore_derivatives() do\n",
    "    #     # for chan_idx = 1:1\n",
    "    #     #     for batch_idx = 1 : num_batches\n",
    "    #     ŷ_cpu_round = round.(ŷ_cpu[:,:, 1, 1])\n",
    "    #     if sum(ŷ_cpu_round) > 0f0\n",
    "    #         ŷ_dtm[:,:, 1, 1] = \n",
    "    #         distance_transform(feature_transform(Bool.(ŷ_cpu_round)))\n",
    "    #     end\n",
    "    #     if sum(y_cpu[:,:, 1, 1]) > 0f0\n",
    "    #         y_dtm[:,:, 1, 1] = \n",
    "    #         distance_transform(feature_transform(Bool.(round.(y_cpu[:,:, 1, 1]))))\n",
    "    #     end\n",
    "    #     #     end\n",
    "    #     # end\n",
    "    # end\n",
    "    # loss_hd = mean(((ŷ_cpu .- y_cpu) .^ 2) .* (ŷ_dtm .^ 4 .+ y_dtm .^ 4))\n",
    "\n",
    "\n",
    "    loss_total = loss_dice * 7f-1 + bce_loss * 3f-1\n",
    "    \n",
    "    ignore_derivatives() do\n",
    "        # log_losses(epoch, step, loss_dice, bce_loss, loss_hd, hd_weight, loss_total, rank+1)\n",
    "        if step <= 321\n",
    "            log_losses(epoch, step, loss_dice, bce_loss, 0f0, 0f0, loss_total, rank+1)\n",
    "        end\n",
    "    end\n",
    "    return loss_total * 25f-2\n",
    "end\n",
    "\n",
    "function bce_dice_and_hd_loss_testmode(ŷ,  y, epoch, step; HD_kick_in = 501, ϵ=1f-5)\n",
    "    ignore_derivatives() do\n",
    "        # x_size, y_size, _, num_batches = size(ŷ)\n",
    "        # hd_weight = min(75f-1, max(0, epoch-HD_kick_in)*1f-2)\n",
    "    \n",
    "        # dice\n",
    "        loss_dice = 1f0 - (muladd(2f0, sum(ŷ .* y), ϵ) / (sum(ŷ) + sum(y) + ϵ))\n",
    "            # 1f0 - (muladd(2f0, sum(ŷ .* y), ϵ) / (sum(ŷ .^ 2) + sum(y .^ 2) + ϵ))\n",
    "    \n",
    "        # binarycrossentropy\n",
    "        # bce_loss = mean(@. -xlogy(y, ŷ .+ ϵ) - xlogy(1f0 .- y, 1f0 .- ŷ .+ ϵ))\n",
    "        # bce_loss = mean(@. -xlogy(y, ŷ + ϵ) - xlogy(1f0 - y, 1f0 - ŷ + ϵ))\n",
    "        bce_loss = weighted_bce_loss(y, ŷ)\n",
    "    \n",
    "        # # HD\n",
    "        # ŷ_dtm = fill(1f3, (x_size, y_size, 1, num_batches))\n",
    "        # y_dtm = fill(1f3, (x_size, y_size, 1, num_batches))\n",
    "        # ŷ_cpu = ŷ |> dev_cpu\n",
    "        # y_cpu = y |> dev_cpu\n",
    "        # ŷ_cpu_round = round.(ŷ_cpu[:,:, 1, 1])\n",
    "        # if sum(ŷ_cpu_round) > 0f0\n",
    "        #     ŷ_dtm[:,:, 1, 1] = \n",
    "        #     distance_transform(feature_transform(Bool.(ŷ_cpu_round)))\n",
    "        # end\n",
    "        # if sum(y_cpu[:,:, 1, 1]) > 0f0\n",
    "        #     y_dtm[:,:, 1, 1] = \n",
    "        #     distance_transform(feature_transform(Bool.(round.(y_cpu[:,:, 1, 1]))))\n",
    "        # end\n",
    "        # loss_hd = mean(((ŷ_cpu .- y_cpu) .^ 2) .* (ŷ_dtm .^ 4 .+ y_dtm .^ 4))\n",
    "        # if step % 10 == 0\n",
    "        #     FluxMPI.fluxmpi_println(\"TESTING step $step / 50\")\n",
    "        # end\n",
    "        # log_losses_testmode(epoch, step, loss_dice, bce_loss, loss_hd, rank+1)\n",
    "        log_losses_testmode(epoch, step, loss_dice, bce_loss, 0f0, rank+1)\n",
    "\n",
    "        # save_path = joinpath(data_dir, \"outputs\", \"epoch#$epoch\",\"test_$(rank+1)_$step.png\")\n",
    "        # save(save_path, Gray.(ŷ_cpu_round))\n",
    "\n",
    "        return loss_dice\n",
    "    end\n",
    "end\n",
    "\n",
    "function compute_loss(x, y, model, ps, st, epoch, step; save_output = true)\n",
    "    # ŷ, st = model(x, ps, st)\n",
    "    \n",
    "    loss = bce_dice_and_hd_loss(ŷ, y, epoch, step, save_output)\n",
    "    return loss, ŷ |> dev_cpu, st\n",
    "end\n",
    "\n",
    "function compute_loss_testmode(x, y, model, ps, st, epoch, step)\n",
    "    # ŷ, st = model(x, ps, st)\n",
    "    ŷ, st = Lux.apply(model, x, ps, st)\n",
    "    dice_loss = bce_dice_and_hd_loss_testmode(ŷ, y, epoch, step)\n",
    "    return dice_loss, ŷ |> dev_cpu, st\n",
    "end\n",
    "\n",
    "function auto_continue_training()\n",
    "    i = 0\n",
    "    while isfile(\"training/saved_train_info_$i.jld2\")\n",
    "        i += 1\n",
    "    end\n",
    "    return max(0, i-1)\n",
    "end\n",
    "\n",
    "# struct Scheduler{T, F}<: Optimisers.AbstractRule\n",
    "#     constructor::F\n",
    "#     schedule::T\n",
    "# end\n",
    "\n",
    "# _get_opt(scheduler::Scheduler, t) = scheduler.constructor(scheduler.schedule(t))\n",
    "\n",
    "# Optimisers.init(o::Scheduler, x::AbstractArray) =\n",
    "#     (t = 1, opt = Optimisers.init(_get_opt(o, 1), x))\n",
    "\n",
    "# function Optimisers.apply!(o::Scheduler, state, x, dx)\n",
    "#     opt = _get_opt(o, state.t)\n",
    "#     new_state, new_dx = Optimisers.apply!(opt, state.opt, x, dx)\n",
    "\n",
    "#     return (t = state.t + 1, opt = new_state), new_dx\n",
    "# end\n",
    "\n",
    "function train(start_epoch_idx, epoch_target, ps, st, opt, st_opt, model, train_loader, test_loader)\n",
    "    FluxMPI.fluxmpi_println(\"Start training...\")\n",
    "    # global ps, st, st_opt\n",
    "\n",
    "    for epoch in start_epoch_idx : epoch_target\n",
    "        η = round(exp(-(epoch-1) * 1f-2) * 1f-2 + 1f-4; digits = 6)\n",
    "        Optimisers.adjust!(st_opt, η)\n",
    "\n",
    "        step = 0\n",
    "        for (x_cpu, y_cpu) in train_loader\n",
    "            step += 1\n",
    "            x, y = x_cpu |> dev, y_cpu |> dev\n",
    "            \n",
    "            (loss, ŷ, st), back = pullback(p -> compute_loss(x, y, model, p, st, epoch, step), ps)\n",
    "\n",
    "            gs = back((one.(loss), nothing, nothing))[1]\n",
    "            # gs = back((one(loss), nothing, nothing))\n",
    "\n",
    "            st_opt, ps = Optimisers.update(st_opt, ps, gs)\n",
    "\n",
    "            # if save_output\n",
    "                SID = split(train_paths[step], \"/\")[8]\n",
    "                save_path = joinpath(data_dir, \"outputs\", \"epoch#$epoch\",\"train_$(rank+1)_$(step)_$(SID).png\")\n",
    "                save(save_path, Gray.(hcat(histogram_equalization(x_cpu[:,:,1,1]), ones(size(ŷ)[1], 1), y_cpu[:,:,1,1], ones(size(ŷ)[1], 1),round.(ŷ[:,:,1,1]))))\n",
    "            # end\n",
    "\n",
    "            CUDA.reclaim()\n",
    "            # if step > 50\n",
    "            #     break\n",
    "            # end\n",
    "        end\n",
    "\n",
    "        # FluxMPI.fluxmpi_println(\"------------------------------------------------------------\")\n",
    "        # save\n",
    "        if rank == 0\n",
    "            # global ps, st\n",
    "            # local \n",
    "            ps_save, st_save = ps |> dev_cpu, st |> dev_cpu\n",
    "            @save joinpath(data_dir, \"saved_train_info_$epoch.jld2\") ps_save st_save\n",
    "        end\n",
    "        # test set\n",
    "        # st_ = Lux.testmode(st)\n",
    "\n",
    "        step = 0\n",
    "        dice_losses = []\n",
    "        for (x_cpu, y_cpu) in test_loader\n",
    "            step += 1\n",
    "            x, y = x_cpu |> dev, y_cpu |> dev\n",
    "            dice_loss, ŷ, _ = compute_loss_testmode(x, y, model, ps, st, epoch, step)\n",
    "            # if save_output\n",
    "                SID = split(test_paths[step], \"/\")[8]\n",
    "                save_path = joinpath(data_dir, \"outputs\", \"epoch#$epoch\",\"test_$(rank+1)_$(step)_$(SID).png\")\n",
    "                save(save_path, Gray.(hcat(histogram_equalization(x_cpu[:,:,1,1]), ones(size(ŷ)[1], 1), y_cpu[:,:,1,1], ones(size(ŷ)[1], 1),round.(ŷ[:,:,1,1]))))\n",
    "            # end\n",
    "\n",
    "            if sum(y_cpu) > 0\n",
    "                push!(dice_losses, dice_loss)\n",
    "            end\n",
    "        end\n",
    "        if epoch % 2 == 0\n",
    "            FluxMPI.fluxmpi_println(\"================== $epoch: η = $η, test set: $(mean(dice_losses)) ==================\")\n",
    "        else\n",
    "            FluxMPI.fluxmpi_println(\"------------------ $epoch: η = $η, test set: $(mean(dice_losses)) ------------------\")\n",
    "        end\n",
    "        CUDA.reclaim()\n",
    "    end\n",
    "end\n",
    "\n",
    "function log_losses(epoch, step, loss_dice, bce_loss, loss_hd, hd_weight, loss_total, id)\n",
    "    filename=\"training/training_log_$id.csv\"\n",
    "    # Check if the file exists\n",
    "    if isfile(filename)\n",
    "        # Load the existing data\n",
    "        df = CSV.File(filename) |> DataFrame\n",
    "    else\n",
    "        # Create a new DataFrame\n",
    "        df = DataFrame(TimeStamp = String[], Epoch = Int[], Step = Int[], LossDice = Float64[], BCELoss = Float64[], LOSS_HD = Float64[], HD_WEIGHT = Float64[], LOSS_TOTAL = Float64[])\n",
    "    end\n",
    "\n",
    "    # Append the new data\n",
    "    new_row = DataFrame(TimeStamp = Dates.format(now(), \"yyyy-mm-dd HH:MM:SS\"), Epoch = epoch, Step = step, LossDice = loss_dice, BCELoss = bce_loss, LOSS_HD = loss_hd, HD_WEIGHT = hd_weight, LOSS_TOTAL = loss_total)\n",
    "    append!(df, new_row)\n",
    "\n",
    "    # Write the updated DataFrame to the CSV file\n",
    "    CSV.write(filename, df)\n",
    "end\n",
    "\n",
    "function log_losses_testmode(epoch, step, loss_dice, bce_loss, loss_hd, id)\n",
    "    filename=\"training/testing_log_$id.csv\"\n",
    "    # Check if the file exists\n",
    "    if isfile(filename)\n",
    "        # Load the existing data\n",
    "        df = CSV.File(filename) |> DataFrame\n",
    "    else\n",
    "        # Create a new DataFrame\n",
    "        df = DataFrame(TimeStamp = String[], Epoch = Int[], Step = Int[], LossDice = Float64[], BCELoss = Float64[], LOSS_HD = Float64[])\n",
    "    end\n",
    "\n",
    "    # Append the new data\n",
    "    new_row = DataFrame(TimeStamp = Dates.format(now(), \"yyyy-mm-dd HH:MM:SS\"), Epoch = epoch, Step = step, LossDice = loss_dice, BCELoss = bce_loss, LOSS_HD = loss_hd)\n",
    "    append!(df, new_row)\n",
    "\n",
    "    # Write the updated DataFrame to the CSV file\n",
    "    CSV.write(filename, df)\n",
    "end\n",
    "\n",
    "isfile(\"training/training_log_$(rank+1).csv\") && rm(\"training/training_log_$(rank+1).csv\")\n",
    "isfile(\"training/testing_log_$(rank+1).csv\") && rm(\"training/testing_log_$(rank+1).csv\")\n",
    "\n",
    "# read data\n",
    "@load joinpath(\"JLD2s/train_loader_$(rank+1).jld2\") data_loader\n",
    "train_loader_ = data_loader\n",
    "\n",
    "@load joinpath(\"JLD2s/test_loader_$(rank+1).jld2\") data_loader\n",
    "test_loader = data_loader\n",
    "\n",
    "@load \"JLD2s/train_dl_paths_$(rank+1).jld2\" paths\n",
    "train_paths = paths\n",
    "\n",
    "@load \"JLD2s/test_dl_paths_$(rank+1).jld2\" paths\n",
    "test_paths = paths\n",
    "\n",
    "# Seeding\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 0)\n",
    "l_r = 1f-3\n",
    "model = UNet(1, 1, 16)\n",
    "\n",
    "# start_epoch_idx = auto_continue_training() # Replace 0 with other epoch idx if training on a saved model\n",
    "start_epoch_idx = 0\n",
    "epoch_target = 500 # end epoch idx\n",
    "# if start_epoch_idx == 0\n",
    "    # train new model \n",
    "    ps, st = Lux.setup(rng, model)\n",
    "\n",
    "    ps = ps |> dev\n",
    "    st = st |> dev\n",
    "\n",
    "    ps = FluxMPI.synchronize!(ps; root_rank = 0)\n",
    "    st = FluxMPI.synchronize!(st; root_rank = 0)\n",
    "    # opt_ = Scheduler(Exp(1f-2, 8f-1)) do lr \n",
    "    #     NAdam(lr) \n",
    "    # end\n",
    "    \n",
    "    # opt = DistributedOptimizer(opt_)\n",
    "    opt = DistributedOptimizer(Adam(l_r))\n",
    "    st_opt = Optimisers.setup(opt, ps)\n",
    "    st_opt = FluxMPI.synchronize!(st_opt; root_rank = 0)\n",
    "    \n",
    "    if rank == 0\n",
    "        local ps_save, st_save = ps |> dev_cpu, st |> dev_cpu\n",
    "        @save joinpath(data_dir, \"saved_train_info_0.jld2\") ps_save st_save\n",
    "    end\n",
    "    train(start_epoch_idx+1, epoch_target, ps, st, opt, st_opt, model, train_loader_, test_loader)\n",
    "# else\n",
    "    # # load saved model \n",
    "    # @load \"saved_train_info_$start_epoch_idx.jld2\" ps_save st_save\n",
    "    # ps = ps_save |> dev\n",
    "    # st = st_save |> dev\n",
    "\n",
    "    # # opt = Scheduler(Exp(1f-2, 8f-1)) do lr \n",
    "    # #     NAdam(lr) \n",
    "    # # end\n",
    "\n",
    "    # opt = DistributedOptimizer(opt)\n",
    "    # st_opt = Optimisers.setup(opt, ps)\n",
    "\n",
    "    # train(start_epoch_idx+1, epoch_target, ps, st, opt, st_opt, model, train_loader_)\n",
    "# end\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
